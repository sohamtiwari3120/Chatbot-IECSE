{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import numpy as np\n",
    "# import tflearn\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import random\n",
    "# import keras\n",
    "import sklearn\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'tag': 'greeting', 'patterns': ['Hi', 'How are you', 'Is anyone there?', 'Hello', 'Good day', 'Whats up'], 'responses': ['Hello!', 'Good to see you again!', 'Hi there, how can I help?'], 'context_set': ''}, {'tag': 'goodbye', 'patterns': ['cya', 'See you later', 'Goodbye', 'I am Leaving', 'Have a Good day'], 'responses': ['Sad to see you go :(', 'Talk to you later', 'Goodbye!'], 'context_set': ''}, {'tag': 'age', 'patterns': ['how old', 'how old is tim', 'what is your age', 'how old are you', 'age?'], 'responses': ['I am 18 years old!', '18 years young!'], 'context_set': ''}, {'tag': 'name', 'patterns': ['what is your name', 'what should I call you', 'whats your name?'], 'responses': ['You can call me Tim.', \"I'm Tim!\", \"I'm Tim aka Tech With Tim.\"], 'context_set': ''}, {'tag': 'shop', 'patterns': ['Id like to buy something', 'whats on the menu', 'what do you reccommend?', 'could i get something to eat'], 'responses': ['We sell chocolate chip cookies for $2!', 'Cookies are on the menu!'], 'context_set': ''}, {'tag': 'hours', 'patterns': ['when are you guys open', 'what are your hours', 'hours of operation'], 'responses': ['We are open 7am-4pm Monday-Friday!'], 'context_set': ''}]\n"
     ]
    }
   ],
   "source": [
    "stemmer = LancasterStemmer()\n",
    "\n",
    "with open(\"intents.json\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "print(data[\"intents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[]\n",
    "labels=[]\n",
    "responses=[]\n",
    "sentences=[]\n",
    "sentence_labels=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intents': [{'tag': 'greeting', 'patterns': ['Hi', 'How are you', 'Is anyone there?', 'Hello', 'Good day', 'Whats up'], 'responses': ['Hello!', 'Good to see you again!', 'Hi there, how can I help?'], 'context_set': ''}, {'tag': 'goodbye', 'patterns': ['cya', 'See you later', 'Goodbye', 'I am Leaving', 'Have a Good day'], 'responses': ['Sad to see you go :(', 'Talk to you later', 'Goodbye!'], 'context_set': ''}, {'tag': 'age', 'patterns': ['how old', 'how old is tim', 'what is your age', 'how old are you', 'age?'], 'responses': ['I am 18 years old!', '18 years young!'], 'context_set': ''}, {'tag': 'name', 'patterns': ['what is your name', 'what should I call you', 'whats your name?'], 'responses': ['You can call me Tim.', \"I'm Tim!\", \"I'm Tim aka Tech With Tim.\"], 'context_set': ''}, {'tag': 'shop', 'patterns': ['Id like to buy something', 'whats on the menu', 'what do you reccommend?', 'could i get something to eat'], 'responses': ['We sell chocolate chip cookies for $2!', 'Cookies are on the menu!'], 'context_set': ''}, {'tag': 'hours', 'patterns': ['when are you guys open', 'what are your hours', 'hours of operation'], 'responses': ['We are open 7am-4pm Monday-Friday!'], 'context_set': ''}]}\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi\n",
      "How are you\n",
      "Is anyone there?\n",
      "Hello\n",
      "Good day\n",
      "Whats up\n",
      "cya\n",
      "See you later\n",
      "Goodbye\n",
      "I am Leaving\n",
      "Have a Good day\n",
      "how old\n",
      "how old is tim\n",
      "what is your age\n",
      "how old are you\n",
      "age?\n",
      "what is your name\n",
      "what should I call you\n",
      "whats your name?\n",
      "Id like to buy something\n",
      "whats on the menu\n",
      "what do you reccommend?\n",
      "could i get something to eat\n",
      "when are you guys open\n",
      "what are your hours\n",
      "hours of operation\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "for intent_dict in data[\"intents\"]:\n",
    "    temp=[]\n",
    "    for sentence in intent_dict[\"patterns\"]:\n",
    "        print(sentence)\n",
    "        w = word_tokenize(sentence)\n",
    "        temp+=w\n",
    "        sentences+=[sentence]\n",
    "        sentence_labels+=[intent_dict[\"tag\"]]\n",
    "    words+=[temp]\n",
    "    if intent_dict[\"tag\"] not in labels:\n",
    "        labels+=[intent_dict[\"tag\"]]\n",
    "    responses+=[intent_dict[\"responses\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Hi', 'How', 'are', 'you', 'Is', 'anyone', 'there', '?', 'Hello', 'Good', 'day', 'Whats', 'up'], ['cya', 'See', 'you', 'later', 'Goodbye', 'I', 'am', 'Leaving', 'Have', 'a', 'Good', 'day'], ['how', 'old', 'how', 'old', 'is', 'tim', 'what', 'is', 'your', 'age', 'how', 'old', 'are', 'you', 'age', '?'], ['what', 'is', 'your', 'name', 'what', 'should', 'I', 'call', 'you', 'whats', 'your', 'name', '?'], ['Id', 'like', 'to', 'buy', 'something', 'whats', 'on', 'the', 'menu', 'what', 'do', 'you', 'reccommend', '?', 'could', 'i', 'get', 'something', 'to', 'eat'], ['when', 'are', 'you', 'guys', 'open', 'what', 'are', 'your', 'hours', 'hours', 'of', 'operation']]\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_flatten=[w for sub_list in words for w in sub_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'How', 'are', 'you', 'Is', 'anyone', 'there', '?', 'Hello', 'Good', 'day', 'Whats', 'up', 'cya', 'See', 'you', 'later', 'Goodbye', 'I', 'am', 'Leaving', 'Have', 'a', 'Good', 'day', 'how', 'old', 'how', 'old', 'is', 'tim', 'what', 'is', 'your', 'age', 'how', 'old', 'are', 'you', 'age', '?', 'what', 'is', 'your', 'name', 'what', 'should', 'I', 'call', 'you', 'whats', 'your', 'name', '?', 'Id', 'like', 'to', 'buy', 'something', 'whats', 'on', 'the', 'menu', 'what', 'do', 'you', 'reccommend', '?', 'could', 'i', 'get', 'something', 'to', 'eat', 'when', 'are', 'you', 'guys', 'open', 'what', 'are', 'your', 'hours', 'hours', 'of', 'operation']\n"
     ]
    }
   ],
   "source": [
    "print(words_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['greeting', 'goodbye', 'age', 'name', 'shop', 'hours']\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greeting\n",
      "\n",
      "['Hi', 'How', 'are', 'you', 'Is', 'anyone', 'there', '?', 'Hello', 'Good', 'day', 'Whats', 'up']\n",
      "\n",
      "['Hello!', 'Good to see you again!', 'Hi there, how can I help?']\n"
     ]
    }
   ],
   "source": [
    "print(f\"{labels[0]}\\n\\n{words[0]}\\n\\n{responses[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_flatten=sorted(list(set([stemmer.stem(w.lower()) for w in words_flatten])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['?', 'a', 'ag', 'am', 'anyon', 'ar', 'buy', 'cal', 'could', 'cya', 'day', 'do', 'eat', 'get', 'good', 'goodby', 'guy', 'hav', 'hello', 'hi', 'hour', 'how', 'i', 'id', 'is', 'lat', 'leav', 'lik', 'menu', 'nam', 'of', 'old', 'on', 'op', 'reccommend', 'see', 'should', 'someth', 'the', 'ther', 'tim', 'to', 'up', 'what', 'when', 'yo', 'you']\n"
     ]
    }
   ],
   "source": [
    "print(words_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": [
    "print(len(words_flatten))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'How are you', 'Is anyone there?', 'Hello', 'Good day', 'Whats up', 'cya', 'See you later', 'Goodbye', 'I am Leaving', 'Have a Good day', 'how old', 'how old is tim', 'what is your age', 'how old are you', 'age?', 'what is your name', 'what should I call you', 'whats your name?', 'Id like to buy something', 'whats on the menu', 'what do you reccommend?', 'could i get something to eat', 'when are you guys open', 'what are your hours', 'hours of operation']\n"
     ]
    }
   ],
   "source": [
    " print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'greeting', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'goodbye', 'age', 'age', 'age', 'age', 'age', 'name', 'name', 'name', 'shop', 'shop', 'shop', 'shop', 'hours', 'hours', 'hours']\n"
     ]
    }
   ],
   "source": [
    "print(sentence_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'goodbye', 'greeting', 'hours', 'name', 'shop']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.sort()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 26\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences), len(sentence_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "enc = LabelBinarizer()\n",
    "sentence_labels_oh = enc.fit_transform(np.array(sentence_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [1 0 0 0 0 0]\n",
      " [1 0 0 0 0 0]\n",
      " [1 0 0 0 0 0]\n",
      " [1 0 0 0 0 0]\n",
      " [1 0 0 0 0 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 0 0 0 0 1]\n",
      " [0 0 0 0 0 1]\n",
      " [0 0 0 0 0 1]\n",
      " [0 0 0 0 0 1]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(sentence_labels_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greeting\t[0 0 1 0 0 0]\n",
      "greeting\t[0 0 1 0 0 0]\n",
      "greeting\t[0 0 1 0 0 0]\n",
      "greeting\t[0 0 1 0 0 0]\n",
      "greeting\t[0 0 1 0 0 0]\n",
      "greeting\t[0 0 1 0 0 0]\n",
      "goodbye\t[0 1 0 0 0 0]\n",
      "goodbye\t[0 1 0 0 0 0]\n",
      "goodbye\t[0 1 0 0 0 0]\n",
      "goodbye\t[0 1 0 0 0 0]\n",
      "goodbye\t[0 1 0 0 0 0]\n",
      "age\t[1 0 0 0 0 0]\n",
      "age\t[1 0 0 0 0 0]\n",
      "age\t[1 0 0 0 0 0]\n",
      "age\t[1 0 0 0 0 0]\n",
      "age\t[1 0 0 0 0 0]\n",
      "name\t[0 0 0 0 1 0]\n",
      "name\t[0 0 0 0 1 0]\n",
      "name\t[0 0 0 0 1 0]\n",
      "shop\t[0 0 0 0 0 1]\n",
      "shop\t[0 0 0 0 0 1]\n",
      "shop\t[0 0 0 0 0 1]\n",
      "shop\t[0 0 0 0 0 1]\n",
      "hours\t[0 0 0 1 0 0]\n",
      "hours\t[0 0 0 1 0 0]\n",
      "hours\t[0 0 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "for i in range(26):\n",
    "    print(f\"{sentence_labels[i]}\\t{sentence_labels_oh[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count vectorizer is used to make BoW ( Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "vectorizer.fit(words_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ag': 0,\n",
       " 'anyon': 1,\n",
       " 'ar': 2,\n",
       " 'buy': 3,\n",
       " 'cal': 4,\n",
       " 'cya': 5,\n",
       " 'day': 6,\n",
       " 'eat': 7,\n",
       " 'good': 8,\n",
       " 'goodby': 9,\n",
       " 'guy': 10,\n",
       " 'hav': 11,\n",
       " 'hello': 12,\n",
       " 'hi': 13,\n",
       " 'hour': 14,\n",
       " 'id': 15,\n",
       " 'lat': 16,\n",
       " 'leav': 17,\n",
       " 'lik': 18,\n",
       " 'menu': 19,\n",
       " 'nam': 20,\n",
       " 'old': 21,\n",
       " 'op': 22,\n",
       " 'reccommend': 23,\n",
       " 'someth': 24,\n",
       " 'ther': 25,\n",
       " 'tim': 26,\n",
       " 'yo': 27}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_stemmed=[]\n",
    "for s in sentences:\n",
    "    w = word_tokenize(s)\n",
    "#     print(w)\n",
    "    temp= \"\"\n",
    "    for j in w:\n",
    "#         print(f\" j ={j}\")\n",
    "        temp+=stemmer.stem(j.lower())+\" \"\n",
    "#         print(temp)\n",
    "    sentences_stemmed+=[temp]\n",
    "#     print(f\"sentences_stemmed = {sentences_stemmed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi ', 'how ar you ', 'is anyon ther ? ', 'hello ', 'good day ', 'what up ', 'cya ', 'see you lat ', 'goodby ', 'i am leav ', 'hav a good day ', 'how old ', 'how old is tim ', 'what is yo ag ', 'how old ar you ', 'ag ? ', 'what is yo nam ', 'what should i cal you ', 'what yo nam ? ', 'id lik to buy someth ', 'what on the menu ', 'what do you reccommend ? ', 'could i get someth to eat ', 'when ar you guy op ', 'what ar yo hour ', 'hour of op ']\n"
     ]
    }
   ],
   "source": [
    "print(sentences_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = vectorizer.fit_transform(sentences_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 13)\t1\n",
      "  (1, 2)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 25)\t1\n",
      "  (3, 12)\t1\n",
      "  (4, 8)\t1\n",
      "  (4, 6)\t1\n",
      "  (6, 5)\t1\n",
      "  (7, 16)\t1\n",
      "  (8, 9)\t1\n",
      "  (9, 17)\t1\n",
      "  (10, 8)\t1\n",
      "  (10, 6)\t1\n",
      "  (10, 11)\t1\n",
      "  (11, 21)\t1\n",
      "  (12, 21)\t1\n",
      "  (12, 26)\t1\n",
      "  (13, 27)\t1\n",
      "  (13, 0)\t1\n",
      "  (14, 2)\t1\n",
      "  (14, 21)\t1\n",
      "  (15, 0)\t1\n",
      "  (16, 27)\t1\n",
      "  (16, 20)\t1\n",
      "  (17, 4)\t1\n",
      "  (18, 27)\t1\n",
      "  (18, 20)\t1\n",
      "  (19, 15)\t1\n",
      "  (19, 18)\t1\n",
      "  (19, 3)\t1\n",
      "  (19, 24)\t1\n",
      "  (20, 19)\t1\n",
      "  (21, 23)\t1\n",
      "  (22, 24)\t1\n",
      "  (22, 7)\t1\n",
      "  (23, 2)\t1\n",
      "  (23, 10)\t1\n",
      "  (23, 22)\t1\n",
      "  (24, 2)\t1\n",
      "  (24, 27)\t1\n",
      "  (24, 14)\t1\n",
      "  (25, 22)\t1\n",
      "  (25, 14)\t1\n"
     ]
    }
   ],
   "source": [
    "print(sentence_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors=sentence_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(sentence_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sentence_vectors.copy()\n",
    "Y_train = sentence_labels_oh.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(sentence_labels_oh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train ,X_test, Y_train, Y_test=train_test_split(X,Y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26, 28) (26, 6)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hi': 13,\n",
       " 'ar': 2,\n",
       " 'anyon': 1,\n",
       " 'ther': 25,\n",
       " 'hello': 12,\n",
       " 'good': 8,\n",
       " 'day': 6,\n",
       " 'cya': 5,\n",
       " 'lat': 16,\n",
       " 'goodby': 9,\n",
       " 'leav': 17,\n",
       " 'hav': 11,\n",
       " 'old': 21,\n",
       " 'tim': 26,\n",
       " 'yo': 27,\n",
       " 'ag': 0,\n",
       " 'nam': 20,\n",
       " 'cal': 4,\n",
       " 'id': 15,\n",
       " 'lik': 18,\n",
       " 'buy': 3,\n",
       " 'someth': 24,\n",
       " 'menu': 19,\n",
       " 'reccommend': 23,\n",
       " 'eat': 7,\n",
       " 'guy': 10,\n",
       " 'op': 22,\n",
       " 'hour': 14}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.Input(shape=X_train.shape[1]),\n",
    "    tf.keras.layers.Dense(8 , activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(8 , activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(Y_train.shape[1], activation='softmax')\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(tf.keras.Input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 8)                 232       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 54        \n",
      "=================================================================\n",
      "Total params: 358\n",
      "Trainable params: 358\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26 samples\n",
      "Epoch 1/200\n",
      "26/26 [==============================] - 1s 22ms/sample - loss: 1.9767 - accuracy: 0.2308\n",
      "Epoch 2/200\n",
      "26/26 [==============================] - 0s 285us/sample - loss: 1.9677 - accuracy: 0.1923\n",
      "Epoch 3/200\n",
      "26/26 [==============================] - 0s 265us/sample - loss: 1.9600 - accuracy: 0.1923\n",
      "Epoch 4/200\n",
      "26/26 [==============================] - 0s 317us/sample - loss: 1.9523 - accuracy: 0.1923\n",
      "Epoch 5/200\n",
      "26/26 [==============================] - 0s 357us/sample - loss: 1.9456 - accuracy: 0.2692\n",
      "Epoch 6/200\n",
      "26/26 [==============================] - 0s 342us/sample - loss: 1.9389 - accuracy: 0.2692\n",
      "Epoch 7/200\n",
      "26/26 [==============================] - 0s 433us/sample - loss: 1.9323 - accuracy: 0.3077\n",
      "Epoch 8/200\n",
      "26/26 [==============================] - 0s 314us/sample - loss: 1.9257 - accuracy: 0.3462\n",
      "Epoch 9/200\n",
      "26/26 [==============================] - 0s 345us/sample - loss: 1.9196 - accuracy: 0.3462\n",
      "Epoch 10/200\n",
      "26/26 [==============================] - 0s 365us/sample - loss: 1.9136 - accuracy: 0.3462\n",
      "Epoch 11/200\n",
      "26/26 [==============================] - 0s 340us/sample - loss: 1.9082 - accuracy: 0.3462\n",
      "Epoch 12/200\n",
      "26/26 [==============================] - 0s 357us/sample - loss: 1.9021 - accuracy: 0.3462\n",
      "Epoch 13/200\n",
      "26/26 [==============================] - 0s 451us/sample - loss: 1.8970 - accuracy: 0.3462\n",
      "Epoch 14/200\n",
      "26/26 [==============================] - 0s 415us/sample - loss: 1.8915 - accuracy: 0.3846\n",
      "Epoch 15/200\n",
      "26/26 [==============================] - 0s 378us/sample - loss: 1.8858 - accuracy: 0.3846\n",
      "Epoch 16/200\n",
      "26/26 [==============================] - 0s 426us/sample - loss: 1.8807 - accuracy: 0.3846\n",
      "Epoch 17/200\n",
      "26/26 [==============================] - 0s 315us/sample - loss: 1.8742 - accuracy: 0.3846\n",
      "Epoch 18/200\n",
      "26/26 [==============================] - 0s 402us/sample - loss: 1.8687 - accuracy: 0.3846\n",
      "Epoch 19/200\n",
      "26/26 [==============================] - 0s 477us/sample - loss: 1.8632 - accuracy: 0.3846\n",
      "Epoch 20/200\n",
      "26/26 [==============================] - 0s 485us/sample - loss: 1.8579 - accuracy: 0.3846\n",
      "Epoch 21/200\n",
      "26/26 [==============================] - 0s 411us/sample - loss: 1.8524 - accuracy: 0.3846\n",
      "Epoch 22/200\n",
      "26/26 [==============================] - 0s 400us/sample - loss: 1.8471 - accuracy: 0.3462\n",
      "Epoch 23/200\n",
      "26/26 [==============================] - 0s 509us/sample - loss: 1.8422 - accuracy: 0.3462\n",
      "Epoch 24/200\n",
      "26/26 [==============================] - 0s 366us/sample - loss: 1.8361 - accuracy: 0.3462\n",
      "Epoch 25/200\n",
      "26/26 [==============================] - 0s 367us/sample - loss: 1.8312 - accuracy: 0.3462\n",
      "Epoch 26/200\n",
      "26/26 [==============================] - 0s 438us/sample - loss: 1.8252 - accuracy: 0.3462\n",
      "Epoch 27/200\n",
      "26/26 [==============================] - 0s 352us/sample - loss: 1.8198 - accuracy: 0.3846\n",
      "Epoch 28/200\n",
      "26/26 [==============================] - 0s 420us/sample - loss: 1.8145 - accuracy: 0.3846\n",
      "Epoch 29/200\n",
      "26/26 [==============================] - 0s 442us/sample - loss: 1.8093 - accuracy: 0.3846\n",
      "Epoch 30/200\n",
      "26/26 [==============================] - 0s 457us/sample - loss: 1.8042 - accuracy: 0.3846\n",
      "Epoch 31/200\n",
      "26/26 [==============================] - 0s 344us/sample - loss: 1.7991 - accuracy: 0.3846\n",
      "Epoch 32/200\n",
      "26/26 [==============================] - 0s 435us/sample - loss: 1.7932 - accuracy: 0.3846\n",
      "Epoch 33/200\n",
      "26/26 [==============================] - 0s 437us/sample - loss: 1.7880 - accuracy: 0.3846\n",
      "Epoch 34/200\n",
      "26/26 [==============================] - 0s 370us/sample - loss: 1.7817 - accuracy: 0.3462\n",
      "Epoch 35/200\n",
      "26/26 [==============================] - 0s 446us/sample - loss: 1.7764 - accuracy: 0.3077\n",
      "Epoch 36/200\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.7945 - accuracy: 0.12 - 0s 416us/sample - loss: 1.7704 - accuracy: 0.3077\n",
      "Epoch 37/200\n",
      "26/26 [==============================] - 0s 306us/sample - loss: 1.7656 - accuracy: 0.3077\n",
      "Epoch 38/200\n",
      "26/26 [==============================] - 0s 393us/sample - loss: 1.7597 - accuracy: 0.3077\n",
      "Epoch 39/200\n",
      "26/26 [==============================] - 0s 411us/sample - loss: 1.7541 - accuracy: 0.3462\n",
      "Epoch 40/200\n",
      "26/26 [==============================] - 0s 349us/sample - loss: 1.7485 - accuracy: 0.3462\n",
      "Epoch 41/200\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.7443 - accuracy: 0.50 - 0s 416us/sample - loss: 1.7424 - accuracy: 0.3462\n",
      "Epoch 42/200\n",
      "26/26 [==============================] - 0s 399us/sample - loss: 1.7371 - accuracy: 0.3462\n",
      "Epoch 43/200\n",
      "26/26 [==============================] - 0s 322us/sample - loss: 1.7312 - accuracy: 0.3077\n",
      "Epoch 44/200\n",
      "26/26 [==============================] - 0s 350us/sample - loss: 1.7249 - accuracy: 0.3077\n",
      "Epoch 45/200\n",
      "26/26 [==============================] - 0s 447us/sample - loss: 1.7193 - accuracy: 0.3077\n",
      "Epoch 46/200\n",
      "26/26 [==============================] - 0s 302us/sample - loss: 1.7138 - accuracy: 0.3077\n",
      "Epoch 47/200\n",
      "26/26 [==============================] - 0s 412us/sample - loss: 1.7082 - accuracy: 0.3077\n",
      "Epoch 48/200\n",
      "26/26 [==============================] - 0s 336us/sample - loss: 1.7026 - accuracy: 0.3462\n",
      "Epoch 49/200\n",
      "26/26 [==============================] - 0s 340us/sample - loss: 1.6966 - accuracy: 0.3462\n",
      "Epoch 50/200\n",
      "26/26 [==============================] - 0s 356us/sample - loss: 1.6915 - accuracy: 0.3462\n",
      "Epoch 51/200\n",
      "26/26 [==============================] - 0s 342us/sample - loss: 1.6855 - accuracy: 0.3846\n",
      "Epoch 52/200\n",
      "26/26 [==============================] - 0s 384us/sample - loss: 1.6795 - accuracy: 0.3846\n",
      "Epoch 53/200\n",
      "26/26 [==============================] - 0s 398us/sample - loss: 1.6734 - accuracy: 0.3846\n",
      "Epoch 54/200\n",
      "26/26 [==============================] - 0s 409us/sample - loss: 1.6666 - accuracy: 0.3462\n",
      "Epoch 55/200\n",
      "26/26 [==============================] - 0s 375us/sample - loss: 1.6604 - accuracy: 0.3462\n",
      "Epoch 56/200\n",
      "26/26 [==============================] - 0s 508us/sample - loss: 1.6547 - accuracy: 0.3462\n",
      "Epoch 57/200\n",
      "26/26 [==============================] - 0s 479us/sample - loss: 1.6477 - accuracy: 0.3462\n",
      "Epoch 58/200\n",
      "26/26 [==============================] - 0s 394us/sample - loss: 1.6414 - accuracy: 0.3462\n",
      "Epoch 59/200\n",
      "26/26 [==============================] - 0s 492us/sample - loss: 1.6346 - accuracy: 0.3462\n",
      "Epoch 60/200\n",
      "26/26 [==============================] - 0s 482us/sample - loss: 1.6280 - accuracy: 0.3846\n",
      "Epoch 61/200\n",
      "26/26 [==============================] - 0s 483us/sample - loss: 1.6212 - accuracy: 0.3846\n",
      "Epoch 62/200\n",
      "26/26 [==============================] - 0s 380us/sample - loss: 1.6146 - accuracy: 0.3846\n",
      "Epoch 63/200\n",
      "26/26 [==============================] - 0s 654us/sample - loss: 1.6082 - accuracy: 0.3846\n",
      "Epoch 64/200\n",
      "26/26 [==============================] - 0s 525us/sample - loss: 1.6023 - accuracy: 0.3846\n",
      "Epoch 65/200\n",
      "26/26 [==============================] - 0s 430us/sample - loss: 1.5961 - accuracy: 0.3846\n",
      "Epoch 66/200\n",
      "26/26 [==============================] - 0s 520us/sample - loss: 1.5897 - accuracy: 0.3846\n",
      "Epoch 67/200\n",
      "26/26 [==============================] - 0s 475us/sample - loss: 1.5827 - accuracy: 0.4231\n",
      "Epoch 68/200\n",
      "26/26 [==============================] - 0s 348us/sample - loss: 1.5764 - accuracy: 0.4231\n",
      "Epoch 69/200\n",
      "26/26 [==============================] - 0s 404us/sample - loss: 1.5683 - accuracy: 0.5000\n",
      "Epoch 70/200\n",
      "26/26 [==============================] - 0s 489us/sample - loss: 1.5612 - accuracy: 0.5000\n",
      "Epoch 71/200\n",
      "26/26 [==============================] - 0s 379us/sample - loss: 1.5530 - accuracy: 0.4615\n",
      "Epoch 72/200\n",
      "26/26 [==============================] - 0s 521us/sample - loss: 1.5459 - accuracy: 0.4615\n",
      "Epoch 73/200\n",
      "26/26 [==============================] - 0s 379us/sample - loss: 1.5373 - accuracy: 0.4615\n",
      "Epoch 74/200\n",
      "26/26 [==============================] - 0s 435us/sample - loss: 1.5299 - accuracy: 0.5000\n",
      "Epoch 75/200\n",
      "26/26 [==============================] - 0s 364us/sample - loss: 1.5220 - accuracy: 0.5000\n",
      "Epoch 76/200\n",
      "26/26 [==============================] - 0s 344us/sample - loss: 1.5146 - accuracy: 0.5000\n",
      "Epoch 77/200\n",
      "26/26 [==============================] - 0s 349us/sample - loss: 1.5066 - accuracy: 0.5000\n",
      "Epoch 78/200\n",
      "26/26 [==============================] - 0s 419us/sample - loss: 1.4985 - accuracy: 0.5385\n",
      "Epoch 79/200\n",
      "26/26 [==============================] - 0s 313us/sample - loss: 1.4917 - accuracy: 0.5000\n",
      "Epoch 80/200\n",
      "26/26 [==============================] - 0s 418us/sample - loss: 1.4827 - accuracy: 0.5000\n",
      "Epoch 81/200\n",
      "26/26 [==============================] - 0s 323us/sample - loss: 1.4759 - accuracy: 0.5000\n",
      "Epoch 82/200\n",
      "26/26 [==============================] - 0s 325us/sample - loss: 1.4680 - accuracy: 0.5385\n",
      "Epoch 83/200\n",
      "26/26 [==============================] - 0s 324us/sample - loss: 1.4598 - accuracy: 0.5385\n",
      "Epoch 84/200\n",
      "26/26 [==============================] - 0s 330us/sample - loss: 1.4529 - accuracy: 0.5385\n",
      "Epoch 85/200\n",
      "26/26 [==============================] - 0s 346us/sample - loss: 1.4446 - accuracy: 0.5385\n",
      "Epoch 86/200\n",
      "26/26 [==============================] - 0s 357us/sample - loss: 1.4366 - accuracy: 0.5385\n",
      "Epoch 87/200\n",
      "26/26 [==============================] - 0s 319us/sample - loss: 1.4293 - accuracy: 0.5769\n",
      "Epoch 88/200\n",
      "26/26 [==============================] - 0s 344us/sample - loss: 1.4224 - accuracy: 0.5769\n",
      "Epoch 89/200\n",
      "26/26 [==============================] - 0s 326us/sample - loss: 1.4141 - accuracy: 0.5769\n",
      "Epoch 90/200\n",
      "26/26 [==============================] - 0s 304us/sample - loss: 1.4067 - accuracy: 0.5769\n",
      "Epoch 91/200\n",
      "26/26 [==============================] - 0s 298us/sample - loss: 1.3990 - accuracy: 0.5769\n",
      "Epoch 92/200\n",
      "26/26 [==============================] - 0s 373us/sample - loss: 1.3919 - accuracy: 0.5769\n",
      "Epoch 93/200\n",
      "26/26 [==============================] - 0s 423us/sample - loss: 1.3833 - accuracy: 0.5769\n",
      "Epoch 94/200\n",
      "26/26 [==============================] - 0s 336us/sample - loss: 1.3757 - accuracy: 0.5769\n",
      "Epoch 95/200\n",
      "26/26 [==============================] - 0s 328us/sample - loss: 1.3681 - accuracy: 0.5769\n",
      "Epoch 96/200\n",
      "26/26 [==============================] - 0s 393us/sample - loss: 1.3597 - accuracy: 0.5769\n",
      "Epoch 97/200\n",
      "26/26 [==============================] - 0s 379us/sample - loss: 1.3519 - accuracy: 0.5769\n",
      "Epoch 98/200\n",
      "26/26 [==============================] - 0s 364us/sample - loss: 1.3445 - accuracy: 0.5769\n",
      "Epoch 99/200\n",
      "26/26 [==============================] - 0s 344us/sample - loss: 1.3356 - accuracy: 0.5769\n",
      "Epoch 100/200\n",
      "26/26 [==============================] - 0s 354us/sample - loss: 1.3283 - accuracy: 0.6154\n",
      "Epoch 101/200\n",
      "26/26 [==============================] - 0s 276us/sample - loss: 1.3189 - accuracy: 0.6154\n",
      "Epoch 102/200\n",
      "26/26 [==============================] - 0s 345us/sample - loss: 1.3093 - accuracy: 0.6154\n",
      "Epoch 103/200\n",
      "26/26 [==============================] - 0s 346us/sample - loss: 1.3016 - accuracy: 0.6154\n",
      "Epoch 104/200\n",
      "26/26 [==============================] - 0s 404us/sample - loss: 1.2937 - accuracy: 0.6154\n",
      "Epoch 105/200\n",
      "26/26 [==============================] - 0s 392us/sample - loss: 1.2850 - accuracy: 0.6154\n",
      "Epoch 106/200\n",
      "26/26 [==============================] - 0s 310us/sample - loss: 1.2772 - accuracy: 0.6154\n",
      "Epoch 107/200\n",
      "26/26 [==============================] - 0s 389us/sample - loss: 1.2687 - accuracy: 0.6154\n",
      "Epoch 108/200\n",
      "26/26 [==============================] - 0s 344us/sample - loss: 1.2609 - accuracy: 0.6154\n",
      "Epoch 109/200\n",
      "26/26 [==============================] - 0s 342us/sample - loss: 1.2523 - accuracy: 0.6154\n",
      "Epoch 110/200\n",
      "26/26 [==============================] - 0s 361us/sample - loss: 1.2443 - accuracy: 0.6154\n",
      "Epoch 111/200\n",
      "26/26 [==============================] - 0s 285us/sample - loss: 1.2370 - accuracy: 0.6538\n",
      "Epoch 112/200\n",
      "26/26 [==============================] - 0s 357us/sample - loss: 1.2282 - accuracy: 0.6538\n",
      "Epoch 113/200\n",
      "26/26 [==============================] - 0s 290us/sample - loss: 1.2205 - accuracy: 0.6538\n",
      "Epoch 114/200\n",
      "26/26 [==============================] - 0s 351us/sample - loss: 1.2121 - accuracy: 0.6538\n",
      "Epoch 115/200\n",
      "26/26 [==============================] - 0s 306us/sample - loss: 1.2040 - accuracy: 0.6538\n",
      "Epoch 116/200\n",
      "26/26 [==============================] - 0s 341us/sample - loss: 1.1958 - accuracy: 0.6538\n",
      "Epoch 117/200\n",
      "26/26 [==============================] - 0s 294us/sample - loss: 1.1878 - accuracy: 0.6538\n",
      "Epoch 118/200\n",
      "26/26 [==============================] - 0s 348us/sample - loss: 1.1794 - accuracy: 0.6538\n",
      "Epoch 119/200\n",
      "26/26 [==============================] - 0s 331us/sample - loss: 1.1717 - accuracy: 0.6538\n",
      "Epoch 120/200\n",
      "26/26 [==============================] - 0s 316us/sample - loss: 1.1638 - accuracy: 0.6538\n",
      "Epoch 121/200\n",
      "26/26 [==============================] - 0s 376us/sample - loss: 1.1560 - accuracy: 0.6538\n",
      "Epoch 122/200\n",
      "26/26 [==============================] - 0s 342us/sample - loss: 1.1476 - accuracy: 0.6538\n",
      "Epoch 123/200\n",
      "26/26 [==============================] - 0s 366us/sample - loss: 1.1399 - accuracy: 0.6538\n",
      "Epoch 124/200\n",
      "26/26 [==============================] - 0s 332us/sample - loss: 1.1313 - accuracy: 0.6538\n",
      "Epoch 125/200\n",
      "26/26 [==============================] - 0s 343us/sample - loss: 1.1239 - accuracy: 0.6538\n",
      "Epoch 126/200\n",
      "26/26 [==============================] - 0s 404us/sample - loss: 1.1166 - accuracy: 0.6538\n",
      "Epoch 127/200\n",
      "26/26 [==============================] - 0s 343us/sample - loss: 1.1082 - accuracy: 0.6923\n",
      "Epoch 128/200\n",
      "26/26 [==============================] - 0s 357us/sample - loss: 1.1002 - accuracy: 0.6538\n",
      "Epoch 129/200\n",
      "26/26 [==============================] - 0s 375us/sample - loss: 1.0935 - accuracy: 0.6538\n",
      "Epoch 130/200\n",
      "26/26 [==============================] - 0s 291us/sample - loss: 1.0862 - accuracy: 0.6538\n",
      "Epoch 131/200\n",
      "26/26 [==============================] - 0s 333us/sample - loss: 1.0795 - accuracy: 0.6538\n",
      "Epoch 132/200\n",
      "26/26 [==============================] - 0s 288us/sample - loss: 1.0726 - accuracy: 0.6538\n",
      "Epoch 133/200\n",
      "26/26 [==============================] - 0s 357us/sample - loss: 1.0657 - accuracy: 0.6923\n",
      "Epoch 134/200\n",
      "26/26 [==============================] - 0s 316us/sample - loss: 1.0583 - accuracy: 0.6923\n",
      "Epoch 135/200\n",
      "26/26 [==============================] - 0s 338us/sample - loss: 1.0513 - accuracy: 0.6923\n",
      "Epoch 136/200\n",
      "26/26 [==============================] - 0s 393us/sample - loss: 1.0437 - accuracy: 0.6923\n",
      "Epoch 137/200\n",
      "26/26 [==============================] - 0s 334us/sample - loss: 1.0364 - accuracy: 0.6923\n",
      "Epoch 138/200\n",
      "26/26 [==============================] - 0s 347us/sample - loss: 1.0292 - accuracy: 0.6923\n",
      "Epoch 139/200\n",
      "26/26 [==============================] - 0s 320us/sample - loss: 1.0231 - accuracy: 0.6923\n",
      "Epoch 140/200\n",
      "26/26 [==============================] - 0s 314us/sample - loss: 1.0160 - accuracy: 0.6923\n",
      "Epoch 141/200\n",
      "26/26 [==============================] - 0s 311us/sample - loss: 1.0087 - accuracy: 0.6923\n",
      "Epoch 142/200\n",
      "26/26 [==============================] - 0s 329us/sample - loss: 1.0019 - accuracy: 0.6923\n",
      "Epoch 143/200\n",
      "26/26 [==============================] - 0s 327us/sample - loss: 0.9954 - accuracy: 0.7308\n",
      "Epoch 144/200\n",
      "26/26 [==============================] - 0s 307us/sample - loss: 0.9889 - accuracy: 0.7308\n",
      "Epoch 145/200\n",
      "26/26 [==============================] - 0s 353us/sample - loss: 0.9822 - accuracy: 0.7308\n",
      "Epoch 146/200\n",
      "26/26 [==============================] - 0s 299us/sample - loss: 0.9758 - accuracy: 0.7308\n",
      "Epoch 147/200\n",
      "26/26 [==============================] - 0s 406us/sample - loss: 0.9695 - accuracy: 0.7308\n",
      "Epoch 148/200\n",
      "26/26 [==============================] - 0s 327us/sample - loss: 0.9635 - accuracy: 0.7308\n",
      "Epoch 149/200\n",
      "26/26 [==============================] - 0s 391us/sample - loss: 0.9575 - accuracy: 0.7308\n",
      "Epoch 150/200\n",
      "26/26 [==============================] - 0s 398us/sample - loss: 0.9512 - accuracy: 0.7308\n",
      "Epoch 151/200\n",
      "26/26 [==============================] - 0s 316us/sample - loss: 0.9450 - accuracy: 0.7308\n",
      "Epoch 152/200\n",
      "26/26 [==============================] - 0s 343us/sample - loss: 0.9391 - accuracy: 0.7308\n",
      "Epoch 153/200\n",
      "26/26 [==============================] - 0s 317us/sample - loss: 0.9329 - accuracy: 0.7308\n",
      "Epoch 154/200\n",
      "26/26 [==============================] - 0s 325us/sample - loss: 0.9269 - accuracy: 0.7308\n",
      "Epoch 155/200\n",
      "26/26 [==============================] - 0s 294us/sample - loss: 0.9206 - accuracy: 0.7308\n",
      "Epoch 156/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 333us/sample - loss: 0.9146 - accuracy: 0.7308\n",
      "Epoch 157/200\n",
      "26/26 [==============================] - 0s 332us/sample - loss: 0.9084 - accuracy: 0.7308\n",
      "Epoch 158/200\n",
      "26/26 [==============================] - 0s 287us/sample - loss: 0.9021 - accuracy: 0.7308\n",
      "Epoch 159/200\n",
      "26/26 [==============================] - 0s 316us/sample - loss: 0.8955 - accuracy: 0.7308\n",
      "Epoch 160/200\n",
      "26/26 [==============================] - 0s 319us/sample - loss: 0.8901 - accuracy: 0.7308\n",
      "Epoch 161/200\n",
      "26/26 [==============================] - 0s 352us/sample - loss: 0.8831 - accuracy: 0.7308\n",
      "Epoch 162/200\n",
      "26/26 [==============================] - 0s 324us/sample - loss: 0.8776 - accuracy: 0.7308\n",
      "Epoch 163/200\n",
      "26/26 [==============================] - 0s 321us/sample - loss: 0.8709 - accuracy: 0.7308\n",
      "Epoch 164/200\n",
      "26/26 [==============================] - 0s 320us/sample - loss: 0.8651 - accuracy: 0.7308\n",
      "Epoch 165/200\n",
      "26/26 [==============================] - 0s 322us/sample - loss: 0.8598 - accuracy: 0.7308\n",
      "Epoch 166/200\n",
      "26/26 [==============================] - 0s 329us/sample - loss: 0.8540 - accuracy: 0.7308\n",
      "Epoch 167/200\n",
      "26/26 [==============================] - 0s 330us/sample - loss: 0.8485 - accuracy: 0.7308\n",
      "Epoch 168/200\n",
      "26/26 [==============================] - 0s 347us/sample - loss: 0.8430 - accuracy: 0.7692\n",
      "Epoch 169/200\n",
      "26/26 [==============================] - 0s 329us/sample - loss: 0.8369 - accuracy: 0.7692\n",
      "Epoch 170/200\n",
      "26/26 [==============================] - 0s 370us/sample - loss: 0.8315 - accuracy: 0.7692\n",
      "Epoch 171/200\n",
      "26/26 [==============================] - 0s 316us/sample - loss: 0.8261 - accuracy: 0.7692\n",
      "Epoch 172/200\n",
      "26/26 [==============================] - 0s 310us/sample - loss: 0.8208 - accuracy: 0.7692\n",
      "Epoch 173/200\n",
      "26/26 [==============================] - 0s 308us/sample - loss: 0.8158 - accuracy: 0.7692\n",
      "Epoch 174/200\n",
      "26/26 [==============================] - 0s 307us/sample - loss: 0.8106 - accuracy: 0.8462\n",
      "Epoch 175/200\n",
      "26/26 [==============================] - 0s 344us/sample - loss: 0.8057 - accuracy: 0.8462\n",
      "Epoch 176/200\n",
      "26/26 [==============================] - 0s 383us/sample - loss: 0.8006 - accuracy: 0.8462\n",
      "Epoch 177/200\n",
      "26/26 [==============================] - 0s 313us/sample - loss: 0.7954 - accuracy: 0.8462\n",
      "Epoch 178/200\n",
      "26/26 [==============================] - 0s 370us/sample - loss: 0.7901 - accuracy: 0.8846\n",
      "Epoch 179/200\n",
      "26/26 [==============================] - 0s 380us/sample - loss: 0.7841 - accuracy: 0.8846\n",
      "Epoch 180/200\n",
      "26/26 [==============================] - 0s 362us/sample - loss: 0.7788 - accuracy: 0.8846\n",
      "Epoch 181/200\n",
      "26/26 [==============================] - 0s 307us/sample - loss: 0.7746 - accuracy: 0.8846\n",
      "Epoch 182/200\n",
      "26/26 [==============================] - 0s 320us/sample - loss: 0.7688 - accuracy: 0.8846\n",
      "Epoch 183/200\n",
      "26/26 [==============================] - 0s 410us/sample - loss: 0.7641 - accuracy: 0.8846\n",
      "Epoch 184/200\n",
      "26/26 [==============================] - 0s 308us/sample - loss: 0.7592 - accuracy: 0.8846\n",
      "Epoch 185/200\n",
      "26/26 [==============================] - 0s 359us/sample - loss: 0.7542 - accuracy: 0.9231\n",
      "Epoch 186/200\n",
      "26/26 [==============================] - 0s 338us/sample - loss: 0.7497 - accuracy: 0.9231\n",
      "Epoch 187/200\n",
      "26/26 [==============================] - 0s 323us/sample - loss: 0.7450 - accuracy: 0.9231\n",
      "Epoch 188/200\n",
      "26/26 [==============================] - 0s 302us/sample - loss: 0.7402 - accuracy: 0.9231\n",
      "Epoch 189/200\n",
      "26/26 [==============================] - 0s 334us/sample - loss: 0.7355 - accuracy: 0.9231\n",
      "Epoch 190/200\n",
      "26/26 [==============================] - 0s 311us/sample - loss: 0.7309 - accuracy: 0.9231\n",
      "Epoch 191/200\n",
      "26/26 [==============================] - 0s 324us/sample - loss: 0.7259 - accuracy: 0.9231\n",
      "Epoch 192/200\n",
      "26/26 [==============================] - 0s 332us/sample - loss: 0.7210 - accuracy: 0.9231\n",
      "Epoch 193/200\n",
      "26/26 [==============================] - 0s 303us/sample - loss: 0.7160 - accuracy: 0.9615\n",
      "Epoch 194/200\n",
      "26/26 [==============================] - 0s 320us/sample - loss: 0.7104 - accuracy: 0.9615\n",
      "Epoch 195/200\n",
      "26/26 [==============================] - 0s 283us/sample - loss: 0.7054 - accuracy: 0.9615\n",
      "Epoch 196/200\n",
      "26/26 [==============================] - 0s 329us/sample - loss: 0.7008 - accuracy: 0.9615\n",
      "Epoch 197/200\n",
      "26/26 [==============================] - 0s 325us/sample - loss: 0.6963 - accuracy: 0.9615\n",
      "Epoch 198/200\n",
      "26/26 [==============================] - 0s 351us/sample - loss: 0.6922 - accuracy: 0.9615\n",
      "Epoch 199/200\n",
      "26/26 [==============================] - 0s 321us/sample - loss: 0.6870 - accuracy: 0.9615\n",
      "Epoch 200/200\n",
      "26/26 [==============================] - 0s 397us/sample - loss: 0.6829 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=X_train, y = Y_train, epochs=200,batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [1.976697481595553, 1.9676528710585375, 1.9600413670906653, 1.9523000166966364, 1.9456395552708552, 1.9388873393719013, 1.9323147443624644, 1.9256793168874888, 1.919626997067378, 1.913562114422138, 1.9082141656142015, 1.9021167296629686, 1.8969718859745905, 1.891498400614812, 1.885767441529494, 1.8806917667388916, 1.8742184547277598, 1.868685089624845, 1.8632248823459332, 1.857896557221046, 1.852406538449801, 1.847080716719994, 1.8421980234292836, 1.8361180195441613, 1.8311848732141347, 1.8252295072262104, 1.8197740224691539, 1.8145106480671809, 1.8093336270405695, 1.8041676741379957, 1.7990943835331843, 1.7931827490146344, 1.787970937215365, 1.7816606943423932, 1.7763689756393433, 1.7704485654830933, 1.7655618282464833, 1.7597366204628577, 1.7541107214414156, 1.7484883345090425, 1.7423878908157349, 1.7370889645356398, 1.731196614412161, 1.7248651156058679, 1.7193129062652588, 1.7138005678470318, 1.7081919358326838, 1.7026165081904485, 1.6966311106315026, 1.6914721085475042, 1.6854760830218976, 1.6795221475454478, 1.6733762392630944, 1.6665740930117094, 1.660437510563777, 1.6546740256823027, 1.6477259947703435, 1.6414330005645752, 1.634575449503385, 1.6279689715458796, 1.6212100432469294, 1.6146042622052705, 1.6082101601820726, 1.602298534833468, 1.5960559386473436, 1.5897366725481474, 1.582734227180481, 1.5763729535616362, 1.5683065469448383, 1.5612482199302087, 1.5530275473227868, 1.5459384734813983, 1.5372731593938975, 1.5298597812652588, 1.5219515745456402, 1.5145978377415583, 1.5065760887586153, 1.4984879860511193, 1.4916500770128691, 1.4826681430523212, 1.475911626448998, 1.4679639339447021, 1.4597643476266127, 1.452900345508869, 1.4446378946304321, 1.4365875170781062, 1.4292923762248113, 1.422423894588764, 1.4140811700087328, 1.4067417016396155, 1.3989812502494225, 1.3919062339342558, 1.3832705387702355, 1.3756634180362408, 1.3680856778071477, 1.3597211654369648, 1.35185641508836, 1.344533186692458, 1.3355721877171443, 1.3283223005441518, 1.3189427394133348, 1.3093335995307336, 1.3016269207000732, 1.2937114788935735, 1.2850124560869658, 1.2772247241093562, 1.2686604353097768, 1.2609119690381563, 1.2523043889265795, 1.2442728189321666, 1.2369824372805083, 1.2282103024996245, 1.2205043480946467, 1.2120682826408973, 1.2039786347976098, 1.195775059553293, 1.1878234881621141, 1.1794122824302087, 1.1716871353296132, 1.1638265297963069, 1.1560209164252648, 1.1475972670775194, 1.1398583283791175, 1.1312729945549598, 1.1238761865175688, 1.1166498844440167, 1.1081645717987647, 1.1002166638007531, 1.093547665155851, 1.0861637592315674, 1.0794606575599084, 1.0726453845317547, 1.0656997240506685, 1.0582553698466375, 1.051321845788222, 1.0436758857506971, 1.0363763295687163, 1.0292061017109797, 1.0230849522810717, 1.0160161348489614, 1.008703818688026, 1.00188475388747, 0.9954304236632127, 0.9888574847808251, 0.9822320296214178, 0.9757522527988141, 0.9694514870643616, 0.963501540514139, 0.9575153589248657, 0.9511699286790994, 0.9450475160892193, 0.9391229748725891, 0.9328668117523193, 0.9268743258256179, 0.9205871728750376, 0.9146264929037827, 0.9083562264075646, 0.9021099072236282, 0.8954739249669589, 0.8900529925639813, 0.8831474230839655, 0.8775776808078473, 0.8709108462700477, 0.8651092992379115, 0.8597830809079684, 0.8539667221216055, 0.8485246163148147, 0.8429807241146381, 0.8368558333470271, 0.8315071028012496, 0.8261183225191556, 0.8207645874757034, 0.8158232111197251, 0.8106361810977643, 0.8056591290694016, 0.8006487717995276, 0.7954392754114591, 0.7901238592771384, 0.7840976623388437, 0.7787825694450965, 0.7746022091462061, 0.7688268331380991, 0.7641084836079524, 0.7591710090637207, 0.7541790833840003, 0.7496512234210968, 0.7450311275628897, 0.7401526570320129, 0.7355184555053711, 0.7308654418358436, 0.7259056751544659, 0.7209647435408372, 0.7160219871080838, 0.7104164361953735, 0.7054240245085496, 0.7007525425690871, 0.6962608213608081, 0.6921603450408349, 0.6869793580128596, 0.682916352382073], 'accuracy': [0.23076923, 0.1923077, 0.1923077, 0.1923077, 0.26923078, 0.26923078, 0.30769232, 0.34615386, 0.34615386, 0.34615386, 0.34615386, 0.34615386, 0.34615386, 0.3846154, 0.3846154, 0.3846154, 0.3846154, 0.3846154, 0.3846154, 0.3846154, 0.3846154, 0.34615386, 0.34615386, 0.34615386, 0.34615386, 0.34615386, 0.3846154, 0.3846154, 0.3846154, 0.3846154, 0.3846154, 0.3846154, 0.3846154, 0.34615386, 0.30769232, 0.30769232, 0.30769232, 0.30769232, 0.34615386, 0.34615386, 0.34615386, 0.34615386, 0.30769232, 0.30769232, 0.30769232, 0.30769232, 0.30769232, 0.34615386, 0.34615386, 0.34615386, 0.3846154, 0.3846154, 0.3846154, 0.34615386, 0.34615386, 0.34615386, 0.34615386, 0.34615386, 0.34615386, 0.3846154, 0.3846154, 0.3846154, 0.3846154, 0.3846154, 0.3846154, 0.3846154, 0.42307693, 0.42307693, 0.5, 0.5, 0.46153846, 0.46153846, 0.46153846, 0.5, 0.5, 0.5, 0.5, 0.53846157, 0.5, 0.5, 0.5, 0.53846157, 0.53846157, 0.53846157, 0.53846157, 0.53846157, 0.5769231, 0.5769231, 0.5769231, 0.5769231, 0.5769231, 0.5769231, 0.5769231, 0.5769231, 0.5769231, 0.5769231, 0.5769231, 0.5769231, 0.5769231, 0.61538464, 0.61538464, 0.61538464, 0.61538464, 0.61538464, 0.61538464, 0.61538464, 0.61538464, 0.61538464, 0.61538464, 0.61538464, 0.65384614, 0.65384614, 0.65384614, 0.65384614, 0.65384614, 0.65384614, 0.65384614, 0.65384614, 0.65384614, 0.65384614, 0.65384614, 0.65384614, 0.65384614, 0.65384614, 0.65384614, 0.65384614, 0.6923077, 0.65384614, 0.65384614, 0.65384614, 0.65384614, 0.65384614, 0.6923077, 0.6923077, 0.6923077, 0.6923077, 0.6923077, 0.6923077, 0.6923077, 0.6923077, 0.6923077, 0.6923077, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7307692, 0.7692308, 0.7692308, 0.7692308, 0.7692308, 0.7692308, 0.7692308, 0.84615386, 0.84615386, 0.84615386, 0.84615386, 0.88461536, 0.88461536, 0.88461536, 0.88461536, 0.88461536, 0.88461536, 0.88461536, 0.9230769, 0.9230769, 0.9230769, 0.9230769, 0.9230769, 0.9230769, 0.9230769, 0.9230769, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 0.96153843, 1.0]}\n"
     ]
    }
   ],
   "source": [
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/soham/anaconda3/envs/tensor_flow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: model Sat Dec 28 11:20:09 2019/assets\n"
     ]
    }
   ],
   "source": [
    "from time import time, ctime\n",
    "# ctime(time())\n",
    "model.save(\"model \"+ctime(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hi': 13, 'ar': 2, 'anyon': 1, 'ther': 25, 'hello': 12, 'good': 8, 'day': 6, 'cya': 5, 'lat': 16, 'goodby': 9, 'leav': 17, 'hav': 11, 'old': 21, 'tim': 26, 'yo': 27, 'ag': 0, 'nam': 20, 'cal': 4, 'id': 15, 'lik': 18, 'buy': 3, 'someth': 24, 'menu': 19, 'reccommend': 23, 'eat': 7, 'guy': 10, 'op': 22, 'hour': 14}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ag', 'anyon', 'ar', 'buy', 'cal', 'cya', 'day', 'eat', 'good', 'goodby', 'guy', 'hav', 'hello', 'hi', 'hour', 'id', 'lat', 'leav', 'lik', 'menu', 'nam', 'old', 'op', 'reccommend', 'someth', 'ther', 'tim', 'yo']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(list(vectorizer.vocabulary_.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start talking with the bot (type quit to stop)!\n",
      "You: quit()\n"
     ]
    }
   ],
   "source": [
    "def bag_of_words(s, dict_word):\n",
    "    bag = [0 for _ in range(len(dict_word))]\n",
    "\n",
    "    s_words = nltk.word_tokenize(s)\n",
    "    s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
    "#     print(s_words)\n",
    "    for se in s_words:\n",
    "        if se in dict_word.keys():\n",
    "            bag[dict_word[se]]=1\n",
    "            \n",
    "    return np.array(bag)\n",
    "\n",
    "\n",
    "def chat(word_dict):\n",
    "    print(\"Start talking with the bot (type quit to stop)!\")\n",
    "    while True:\n",
    "        inp = input(\"You: \")\n",
    "        if \"quit\" in inp.lower():\n",
    "            break\n",
    "#         print(bag_of_words(inp, word_dict).shape, bag_of_words(inp, word_dict))\n",
    "        results = model.predict(bag_of_words(inp, word_dict).reshape(1,-1))\n",
    "        results_index = np.argmax(results)\n",
    "        tag = labels[results_index]\n",
    "\n",
    "        for tg in data[\"intents\"]:\n",
    "            if tg['tag'] == tag:\n",
    "                responses = tg['responses']\n",
    "\n",
    "        print(np.random.choice(responses))\n",
    "\n",
    "chat(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensor_flow]",
   "language": "python",
   "name": "conda-env-tensor_flow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
